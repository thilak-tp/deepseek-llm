# Deepseek LLM
This repository will contain me exploring the Deepseek LLM and different ways to use this powerfull open source LLM
## 1. To run a deepseek model on your computer
   You can use Ollama or LM Studio to choose and load your preferred LLM model and run it locally on your computer.
### Ollama
<img src="resources/ollama.png" width="400" />
   1. Install Ollama and open it on the command prompt or the terminal.
   2. Go to their website and choose an LLM model, copy that model name and then
      use ollama run model-name-as-per-website
   3. This will open a command line interface where you can interact with the model.

### LM Studio
<img src="resources/lmstudio.png" width="400" />
  It is a GUI LLM model runner which has a better interface, on par with other LLM chat interfaces.
  1. Install LM Studio and then in the app, choose the LLM model that you want to run.


# Ollama
## Deepseek R1 7B version 
<img src="resources/r1_7b_vram_usage.png"width="400" />
As seen the 7B parameter version uses around 5Gigs of VRAM on my NVIDIA RTX 3070 Ti Mobile GPU which has 8Gigs of VRAM
